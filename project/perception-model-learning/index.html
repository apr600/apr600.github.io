<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
    
    
    
    
  
  

  <meta name="author" content="Ahalya Prabhakar">

  
  
  
    
  
  <meta name="description" content="As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable.">

  
  <link rel="alternate" hreflang="en-us" href="https://apr600.github.io/project/perception-model-learning/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css" integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin="anonymous">
    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.3b22c150b33731e873cb2255c65de711.css">

  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://apr600.github.io/project/perception-model-learning/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Academic">
  <meta property="og:url" content="https://apr600.github.io/project/perception-model-learning/">
  <meta property="og:title" content="Learning Interpretable Action-Perception Models | Academic">
  <meta property="og:description" content="As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable."><meta property="og:image" content="https://apr600.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://apr600.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-03T12:07:34-05:00">
    
    <meta property="article:modified_time" content="2019-09-03T12:07:34-05:00">
  

  


    










  





  


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://apr600.github.io/project/perception-model-learning/"
  },
  "headline": "Learning Interpretable Action-Perception Models",
  
  "datePublished": "2019-09-03T12:07:34-05:00",
  "dateModified": "2019-09-03T12:07:34-05:00",
  
  "author": {
    "@type": "Person",
    "name": "Ahalya Prabhakar"
  },
  
  "description": "As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable."
}
</script>

  

  


  


  





  <title>Learning Interpretable Action-Perception Models | Academic</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Academic</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        

      </ul>

    </div>
  </div>
</nav>


  <article class="article article-project">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Learning Interpretable Action-Perception Models</h1>

  

  
    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Sep 3, 2019
  </span>
  

  

  

  
  
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://apr600.github.io/project/perception-model-learning/&amp;text=Learning%20Interpretable%20Action-Perception%20Models" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://apr600.github.io/project/perception-model-learning/&amp;t=Learning%20Interpretable%20Action-Perception%20Models" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Learning%20Interpretable%20Action-Perception%20Models&amp;body=https://apr600.github.io/project/perception-model-learning/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://apr600.github.io/project/perception-model-learning/&amp;title=Learning%20Interpretable%20Action-Perception%20Models" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Learning%20Interpretable%20Action-Perception%20Models%20https://apr600.github.io/project/perception-model-learning/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://apr600.github.io/project/perception-model-learning/&amp;title=Learning%20Interpretable%20Action-Perception%20Models" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable. Furthermore, in high-dimensional sensor systems, I explore how we can efficiently acquire and utilize the data for model learning and communication.</p>
<h2 id="multimodal-sensory-learning-for-real-time-adaptive-manipulation">Multimodal Sensory Learning for Real-time Adaptive Manipulation</h2>
<p>Adaptive control for real-time manipulation requires quick estimation and prediction of object properties. While robot
learning in this area primarily focuses on using vision, many tasks cannot rely on vision due to object occlusion. I focus on developing methods for using multimodal sensory fusion of tactile and audio data to quickly characterize and predict an object&rsquo;s tactile force during motion dependent on their inertial properties. The predictions are used in a adaptive grasp controller to compensate for the predicted inertial forces experienced during motion. Drawing inspiration from how humans interact with objects, I investigate how to best utilize different sensory signals and actively interact with and manipulate objects to quickly learn their object properties for safe manipulation. By exploring the information content and quality in different sensory modalities, I develop algorithms that autonomously extract the relevant task information to maintain grasp stability during manipulation.</p>
<p>For more information, check out the <a href="/publication/multimodal-manip-esn/">paper</a> on this work.</p>
<h2 id="mechanical-intelligence-for-learning-embodied-sensor-object-relationships">Mechanical intelligence for learning embodied sensor-object relationships</h2>
<p>My work explores the idea of active learning for high-dimensional sensory spaces. Understanding sensory experiences and building contextual representations without prior knowledge of sensor models and environment is a challenging unsupervised learning problem. This is particularly challenging for robotics because the data has to be physically acquired and affects the learning process. I develop a method that enables agents to efficiently collect data for learning a predictive sensor model—without requiring domain knowledge, human input, or previously existing data—using ergodicity to specify the data acquisition process. This approach is based entirely on data-driven sensor characteristics rather than predefined knowledge of the sensor model and its physical characteristics.</p>
<p>For more information, check out the <a href="/publication/mechanical-intelligence/">paper</a> on this work.</p>
<h3 id="active-exploration-for-learning-haptic-features">Active Exploration for Learning Haptic Features</h3>
<!-- raw HTML omitted -->
<p>My work explores the idea of active learning for tactile exploration. Tactile exploration is a particularly interesting example of high-dimensional sensory exploration because, unlike vision, the exploratory motions used to obtain the data affect the data itself. That is, <em>how you move across the texture affects the sensory data received</em>. As such, learning and classifying texture data from exploratory motions can be a challenging problem.</p>
<p>Most of the work in tactile exploration either focus on learning to translate the sensory signals to physically interpretable characteristics. Others simplify the tactile exploration for generating discriminatory motions by either mimicking human exploratory motions or by building a library of motions to discriminate particular characteristics. The question I am exploring is, <em>Can I learn textures and mappings of workspaces directly from the high-dimensional tactile sensory signals without trying to associate physical characteristics to the sensory signals?</em></p>
<p>Using the Biotac tactile sensor from SynTouch, Inc., a 19-dimensional tactile sensor that mimics the sensory capabilities of the human fingertip, I explore the use of unsupervised learning methods to learn, model, and classify textures without a prior library of texture definitions or predefined exploratory motions.</p>
<p>For more information, check out the <a href="/publication/tactile-learning-icra/">paper</a> on this work.</p>
<h3 id="distribution-based-active-exploration-and-the-kl-divergence-measure">Distribution-Based Active Exploration and the KL-Divergence Measure</h3>






<figure>

  <a data-fancybox="" href="quadcopter_learning.gif" >

<img src="quadcopter_learning.gif" width="240" height="253" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>KL-E3 quadcopter (in blue) actively acquiring data to learn its stochastic model compared to a quadcopter acquiring data using an information-maximizing method (in green).</h4>
  
</figcaption>

</figure>

<p>K-L divergence based control method focused on efficiently exploring high-dimensional search spaces safely for active, stable exploration. For efficient learning, we want to drive the exploration of the robot to prioritize obtaining more informative data. To do this, we defined an information distribution over the search space that represented the importance measure of the data (such as measures of linear independence from sparse Gaussian processes or measures representing model parameter uncertainty with respect to the search space). By defining the importance measure based on the state space, we generated a representation of the state space that prioritized states that generate highly informative data.</p>






<figure>

  <a data-fancybox="" href="distr_match.gif" >

<img src="distr_match.gif" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>Using the KL-E3 method to actively acquire data to learn and match a distribution.</h4>
  
</figcaption>

</figure>

<p>We then used information-based measures with hybrid control to generate controls that cover the search space and prioritizing high-information regions by matching the information distribution generated, while maintaining safety and stability constraints. The ergodic metric allows us to quantitatively define a measure that minimizes the distance between the information distribution and the time-averaged trajectory of the robot. Using it in the cost function allows us to generate controls that explore the full state-space while prioritizing high-information regions. The main limitation of the ergodic metric used in the active exploration algorithm is that the computational complexity of ergodicity exponentially increases with the dimensionality of the search space. To address this, we derive an active exploration algorithm using a sampling-based approximation of the ergodic metric using the Kullback-Leibler divergence metric. The sampling-based approximation allowed us to actively explore a much higher dimensional search space without sacrificing real-time control.</p>
<p>For more information about this project, check out the papers on this work <a href="/publication/kl-e3-tase/">here</a> and <a href="/publication/kl-e3-wafr/">here</a> and its <a href="https://github.com/i-abr/kle3">Github repository</a>.</p>






<figure>

  <a data-fancybox="" href="anim.gif" >

<img src="anim.gif" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>A robot using the KL-E3 method to sampling a state space (for Bayesian optimization) while taking into account dynamic constraints (keeping the cart double pendulum inverted).</h4>
  
</figcaption>

</figure>


    </div>

    


    








  






  
  
  
    
  
  
  <div class="media author-card">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_hu823182955dadc71e57986238344cec64_319097_250x250_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://apr600.github.io/">Ahalya Prabhakar</a></h5>
      <h6 class="card-subtitle">Lecturer and Associate Research Scientist</h6>
      <p class="card-text">My research interests include robot active learning from high-dimensional sensory signals and human-robot interaction using information-theoretic algorithms.</p>
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
          <li>
            <a href="mailto:ahalya.prabhakar@gmail.com" >
              <i class="fas fa-envelope"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a href="https://scholar.google.com/citations?user=7jk2mfMAAAAJ&amp;hl=en" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a href="https://github.com/apr600" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>




    


    
    

    
    
    

    
    
    
      <h2>Publications</h2>
      
        
          








  
  





  


<div class="media stream-item">
  <div class="media-body">

    <h3 class="article-title mb-0 mt-0">
      <a href="/publication/tactile-learning-icra/" >Active Exploration for Real-Time Haptic Training</a>
    </h3>

    
    <div class="article-style">
      Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which …
    </div>
    

    <div class="stream-meta article-metadata">

      

      
      <div>
        



  <span><a href="/authors/j.-ketchum/">J. Ketchum</a></span>, <span><a href="/authors/a.-prabhakar/">A. Prabhakar</a></span>, <span><a href="/authors/t.-d.-murphey/">T. D. Murphey</a></span>

      </div>
      
    </div>

    
    <div class="btn-links">
      








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://arxiv.org/abs/2405.11776" target="_blank" rel="noopener">
  PDF
</a>







  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/perception-model-learning/">
    Project
  </a>
  











    </div>
    

  </div>
  <div class="ml-3">
    
    
  </div>
</div>

        
      
    

    
    
    

  </div>
</article>



      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js" integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin="anonymous"></script>
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.59ebf34902d7a2a1bb85a85422b3e846.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
