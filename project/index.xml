<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Academic</title>
    <link>https://apr600.github.io/project/</link>
      <atom:link href="https://apr600.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 03 Sep 2019 12:07:34 -0500</lastBuildDate>
    <image>
      <url>https://apr600.github.io/img/icon-192.png</url>
      <title>Projects</title>
      <link>https://apr600.github.io/project/</link>
    </image>
    
    <item>
      <title>Learning Interpretable Action-Perception Models</title>
      <link>https://apr600.github.io/project/perception-model-learning/</link>
      <pubDate>Tue, 03 Sep 2019 12:07:34 -0500</pubDate>
      <guid>https://apr600.github.io/project/perception-model-learning/</guid>
      <description>&lt;p&gt;As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable. Furthermore, in high-dimensional sensor systems, I explore how we can efficiently acquire and utilize the data for model learning and communication.&lt;/p&gt;
&lt;h2 id=&#34;multimodal-sensory-learning-for-real-time-adaptive-manipulation&#34;&gt;Multimodal Sensory Learning for Real-time Adaptive Manipulation&lt;/h2&gt;
&lt;p&gt;Adaptive control for real-time manipulation requires quick estimation and prediction of object properties. While robot
learning in this area primarily focuses on using vision, many tasks cannot rely on vision due to object occlusion. I focus on developing methods for using multimodal sensory fusion of tactile and audio data to quickly characterize and predict an object&amp;rsquo;s tactile force during motion dependent on their inertial properties. The predictions are used in a adaptive grasp controller to compensate for the predicted inertial forces experienced during motion. Drawing inspiration from how humans interact with objects, I investigate how to best utilize different sensory signals and actively interact with and manipulate objects to quickly learn their object properties for safe manipulation. By exploring the information content and quality in different sensory modalities, I develop algorithms that autonomously extract the relevant task information to maintain grasp stability during manipulation.&lt;/p&gt;
&lt;p&gt;For more information, check out the &lt;a href=&#34;https://apr600.github.io/publication/multimodal-manip-esn/&#34;&gt;paper&lt;/a&gt; on this work.&lt;/p&gt;
&lt;h2 id=&#34;mechanical-intelligence-for-learning-embodied-sensor-object-relationships&#34;&gt;Mechanical intelligence for learning embodied sensor-object relationships&lt;/h2&gt;
&lt;p&gt;My work explores the idea of active learning for high-dimensional sensory spaces. Understanding sensory experiences and building contextual representations without prior knowledge of sensor models and environment is a challenging unsupervised learning problem. This is particularly challenging for robotics because the data has to be physically acquired and affects the learning process. I develop a method that enables agents to efficiently collect data for learning a predictive sensor model—without requiring domain knowledge, human input, or previously existing data—using ergodicity to specify the data acquisition process. This approach is based entirely on data-driven sensor characteristics rather than predefined knowledge of the sensor model and its physical characteristics.&lt;/p&gt;
&lt;p&gt;For more information, check out the &lt;a href=&#34;https://apr600.github.io/publication/mechanical-intelligence/&#34;&gt;paper&lt;/a&gt; on this work.&lt;/p&gt;
&lt;h3 id=&#34;active-exploration-for-learning-haptic-features&#34;&gt;Active Exploration for Learning Haptic Features&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;My work explores the idea of active learning for tactile exploration. Tactile exploration is a particularly interesting example of high-dimensional sensory exploration because, unlike vision, the exploratory motions used to obtain the data affect the data itself. That is, &lt;em&gt;how you move across the texture affects the sensory data received&lt;/em&gt;. As such, learning and classifying texture data from exploratory motions can be a challenging problem.&lt;/p&gt;
&lt;p&gt;Most of the work in tactile exploration either focus on learning to translate the sensory signals to physically interpretable characteristics. Others simplify the tactile exploration for generating discriminatory motions by either mimicking human exploratory motions or by building a library of motions to discriminate particular characteristics. The question I am exploring is, &lt;em&gt;Can I learn textures and mappings of workspaces directly from the high-dimensional tactile sensory signals without trying to associate physical characteristics to the sensory signals?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Using the Biotac tactile sensor from SynTouch, Inc., a 19-dimensional tactile sensor that mimics the sensory capabilities of the human fingertip, I explore the use of unsupervised learning methods to learn, model, and classify textures without a prior library of texture definitions or predefined exploratory motions.&lt;/p&gt;
&lt;p&gt;For more information, check out the &lt;a href=&#34;https://apr600.github.io/publication/tactile-learning-icra/&#34;&gt;paper&lt;/a&gt; on this work.&lt;/p&gt;
&lt;h3 id=&#34;distribution-based-active-exploration-and-the-kl-divergence-measure&#34;&gt;Distribution-Based Active Exploration and the KL-Divergence Measure&lt;/h3&gt;






&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;quadcopter_learning.gif&#34; &gt;

&lt;img src=&#34;quadcopter_learning.gif&#34; width=&#34;240&#34; height=&#34;253&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;KL-E3 quadcopter (in blue) actively acquiring data to learn its stochastic model compared to a quadcopter acquiring data using an information-maximizing method (in green).&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;K-L divergence based control method focused on efficiently exploring high-dimensional search spaces safely for active, stable exploration. For efficient learning, we want to drive the exploration of the robot to prioritize obtaining more informative data. To do this, we defined an information distribution over the search space that represented the importance measure of the data (such as measures of linear independence from sparse Gaussian processes or measures representing model parameter uncertainty with respect to the search space). By defining the importance measure based on the state space, we generated a representation of the state space that prioritized states that generate highly informative data.&lt;/p&gt;






&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;distr_match.gif&#34; &gt;

&lt;img src=&#34;distr_match.gif&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Using the KL-E3 method to actively acquire data to learn and match a distribution.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;We then used information-based measures with hybrid control to generate controls that cover the search space and prioritizing high-information regions by matching the information distribution generated, while maintaining safety and stability constraints. The ergodic metric allows us to quantitatively define a measure that minimizes the distance between the information distribution and the time-averaged trajectory of the robot. Using it in the cost function allows us to generate controls that explore the full state-space while prioritizing high-information regions. The main limitation of the ergodic metric used in the active exploration algorithm is that the computational complexity of ergodicity exponentially increases with the dimensionality of the search space. To address this, we derive an active exploration algorithm using a sampling-based approximation of the ergodic metric using the Kullback-Leibler divergence metric. The sampling-based approximation allowed us to actively explore a much higher dimensional search space without sacrificing real-time control.&lt;/p&gt;
&lt;p&gt;For more information about this project, check out the papers on this work &lt;a href=&#34;https://apr600.github.io/publication/kl-e3-tase/&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://apr600.github.io/publication/kl-e3-wafr/&#34;&gt;here&lt;/a&gt; and its &lt;a href=&#34;https://github.com/i-abr/kle3&#34;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;






&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;anim.gif&#34; &gt;

&lt;img src=&#34;anim.gif&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;A robot using the KL-E3 method to sampling a state space (for Bayesian optimization) while taking into account dynamic constraints (keeping the cart double pendulum inverted).&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Optimal Interface Design for Human-Robot Collaboration</title>
      <link>https://apr600.github.io/project/human-swarm-collab/</link>
      <pubDate>Tue, 03 Sep 2019 12:07:14 -0500</pubDate>
      <guid>https://apr600.github.io/project/human-swarm-collab/</guid>
      <description>&lt;p&gt;For robot interfaces to be practical for humans, they must be simple, seamless, and mitigate cognitive load for the user. This is particularly critical for users who may not have familiarity with robotic systems or in settings with complex collaborative robotic systems (e.g., robot swarms) or dynamically changing environments, where managing the system can be overwhelming.
In my research, I seek to design user interfaces for human-robot collaboration that effectively communicates information to the human with minimal cognitive load and training to use.&lt;/p&gt;
&lt;h2 id=&#34;ergodic-specifications-for-flexible-swarm-control-and-dynamic-task-adaptation&#34;&gt;Ergodic Specifications for Flexible Swarm Control and Dynamic Task Adaptation&lt;/h2&gt;
&lt;p&gt;People often have to perform a variety of tasks-including search-and-rescue, target location, or exploration and terrain mapping- in unfamiliar environments. Drones deployed in the field with them have the ability to greatly improve their situational and perceptual awareness by providing feedback to aid in their task performance and safety. However, swarm deployment can be complicated, as drones can be difficult for a person to control and operate without greatly increasing their cognitive load, which can make task performance difficult and inefficient. With this in mind, I ask the question: &lt;em&gt;How can we design a human-swarm system to best accomplish a task under pressure?&lt;/em&gt;&lt;/p&gt;






&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;comb_sim.png&#34; &gt;

&lt;img src=&#34;comb_sim.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Simulation of a swarm dynamically adapting to the environment while also responding to user commands. (a) When the swarm discovers a DD, the agents cover the rest of the workspace while avoiding that location. (b) When a user inputs a bimodal distribution (shown as the dark region on the map), the swarm responds to the user commands, while continuing to avoid the DD location. (c) When the swarm finds an EE, it simultaneously converges on the EE, covers the user inputs, and avoids the DD location.  (d) shows the resulting target distribution for the combined tasks.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;I led the Northwestern team on the DARPA FX-3 Urban Swarm Challenge project, exploring this idea. I developed a formulation for swarm control and high-level task planning that is dynamically responsive to user commands and adaptable to environmental information. I designed an end-to-end pipeline from a tactile tablet interface for user commands to onboard control of robotic agents based on decentralized ergodic coverage. I conducted experiments with a robotic swarm at the DARPA OFFSET FX3 field tests, combining user inputs and task specifications to generate swarm behavior flexible to changing conditions and objectives in real-time. I also developed an experimental VR test bed to validate our approach in a simulation and conduct human subject studies investigating human-robot collaboration under pressure.&lt;/p&gt;






&lt;figure&gt;

  &lt;a data-fancybox=&#34;&#34; href=&#34;tanvas_pic_cropped.png&#34; &gt;

&lt;img src=&#34;tanvas_pic_cropped.png&#34; &gt;
&lt;/a&gt;


&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;The Tanvas tactile tablet allows the operator to communicate their preferences for coverage to the swarm.&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;User inputs are incorporated through a tablet interface we developed for communicating regions of interest by the user to a swarm in realtime using the TanvasTouch monitor. Using the TanvasTouch, the user can specify regions of exploratory interest by simply shading the regions of interest on the TanvasTouch. The tablet interface transmits a set of desired points on the workspace for the swarm to prioritize and the spatial distribution is generated by assigning the highest priority value at each of those points in a discretized workspace. The user-specified distribution is combined  with the task-based distribution to generate swarm control that is dynamically responsive to both task updates and user inputs.&lt;/p&gt;
&lt;p&gt;For more information, check out the following papers (&lt;a href=&#34;https://apr600.github.io/publication/darpa-fx3-rss/&#34;&gt;paper1&lt;/a&gt;, &lt;a href=&#34;https://apr600.github.io/publication/scale-invariant-darpa/&#34;&gt;paper2&lt;/a&gt;)  on this work.&lt;/p&gt;
&lt;h3 id=&#34;virtual-reality-system-for-human-subject-studies&#34;&gt;Virtual Reality System for Human-Subject Studies&lt;/h3&gt;
&lt;p&gt;We developed an experimental urban environment testbed using the Unity game engine and using an HTC Vive for controlling operator movement inside the virtual reality environment. As the operator moved within the VR environment, a swarm of simulated quadrotors running the ergodic swarm algorithm provided assisstane to the operator in real time.The swarm&amp;rsquo;s behavior was governed by ergodic task specifications as described above, as well as using the TanvasTouch haptic interface to send commands for desired areas of explorations to the swarm.&lt;/p&gt;
&lt;p&gt;The VR environment was used to validate the full system architecture of the shared ergodic formulation with multiple two-way communication channels. In addition, we have conducted user studies using this testbed to analyze the impact of the shared ergodic swarm control approach compared to standard methods and analyzes the effects of different ergodic specifications within our framework on task performance under dynamic, time-sensitive constraints.&lt;/p&gt;
&lt;p&gt;For more information, check out the following papers (&lt;a href=&#34;https://apr600.github.io/publication/swarm-hsr-pnas/&#34;&gt;paper1&lt;/a&gt;, &lt;a href=&#34;https://apr600.github.io/publication/swarm-hsr-iros/&#34;&gt;paper2&lt;/a&gt;)  on this work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Principled Methods for Human-Robot Collaborative Learning and Control</title>
      <link>https://apr600.github.io/project/task-rep/</link>
      <pubDate>Tue, 03 Sep 2019 12:04:05 -0500</pubDate>
      <guid>https://apr600.github.io/project/task-rep/</guid>
      <description>&lt;p&gt;Enabling seamless human-robot collaboration while ensuring task success requires the reduction of the task information to its essential features through representations that are intuitive for the user. To do this, I focus on developing methods for generating and learning task representations from motion data and information that can be incorporated into model-based control methods.
My goal is to develop methods that enable autonomy to extract understandable task embeddings that enable control methods to ensure dynamic feasibility and provably safe behavior and can be communicated to and interpreted by humans, a step in developing explainable AI.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;credit-assignment-safety-learning-from-suboptimal-and-failure-demonstrations&#34;&gt;Credit Assignment Safety Learning from Suboptimal and Failure Demonstrations&lt;/h2&gt;
&lt;p&gt;A critical need in assistive robotics is to learn task intent and
safety guarantees through user interactions in order to ensure safe task performance. Most robot learning from demonstration (LfD) and inverse reinforcement learning (IRL) methods primarily rely on optimal demonstration in order to successfully learn a control policy, which can be challenging to acquire from novice users. Recent work does use suboptimal and failed demonstrations to learn about task intent; few focus on learning safety guarantees to prevent repeat failures experienced, essential for assistive robots. Furthermore, interactive human-robot learning aims to minimize effort from the human user to facilitate deployment in the real-world. As such, requiring users to label the unsafe states or keyframes from the demonstrations should not be a necessary requirement for learning. I developed an algorithm to learn a safety value function from a set of suboptimal and failed demonstrations that is used to generate a real-time safety control filter. Importantly, we develop a credit assignment method that extracts the failure states from the failed demonstrations without requiring human labelling or prespecified knowledge of unsafe regions. This method can be combined with standard LfD or IRL methods to learn a task policy that also guarantees safety during execution.&lt;/p&gt;
&lt;p&gt;For more information about this project, check out the &lt;a href=&#34;https://arxiv.org/abs/2110.04633&#34;&gt;paper&lt;/a&gt; or my &lt;a href=&#34;https://youtu.be/eEBVH1-BpfI&#34;&gt;talk&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;learning-from-variable-imperfect-demonstrations-using-ergodic-control&#34;&gt;Learning from Variable, Imperfect Demonstrations using Ergodic Control&lt;/h2&gt;
&lt;p&gt;As robots become more ubiquitous in every day life, they will be interacting with people more and will need to learn from them. At the same time, as regular people are required to teach robots to perform more challenging tasks and provide demonstrations for complicated robotic systems that they may be unfamiliar with, they may provide demonstrations that are non-optimal or even unsuccessful. &lt;em&gt;How do we allow robots to learn a task representation from a set of human demonstrations that 1) can encompass varied solutions to the same task, and, perhaps more importantly, 2) can still generate optimal solutions from a set of imperfect demonstrations from a non-expert user?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here, I explore the idea of representing the set of demonstrations as an information distribution over the task space. By considering each demonstration as adding information about the task, we can consider imperfect, and even unsuccessful, demonstrations as still adding valuable information about the task to the representation. As such, a set of imperfect demonstrations can collectively create a task representation from which we can generate controls for optimal task performance. Furthermore, this representation allows more flexibility in the demonstration set, allowing for multiple solution sets for the same task. By representing the task as information over the task space, multiple solutions can emerge from a demonstration set and variations that are irrelevant to task success will naturally be averaged out.&lt;/p&gt;
&lt;p&gt;For more information about this project, check out the &lt;a href=&#34;https://arxiv.org/abs/2103.17098&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;robotic-visual-rendering-using-information-theoretic-methods&#34;&gt;Robotic Visual Rendering using Information-Theoretic Methods&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/h2wnD6e4grw?autoplay=1&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Drawing is a classic example of tasks where the motions are used to successfully accomplish the task of communicating information. It falls into a unique subset of tasks where the motion is simultaneously essential for accomplishing the task, but there are many motion trajectories that can successfully accomplish it&amp;mdash; people may draw the same image completely different, but they ultimately result in the same image. I wanted to give the same level of robustness and generality to a robot&amp;rsquo;s task performance.&lt;/p&gt;
&lt;p&gt;To do this, I represent the task as the distribution over the state space representing the relevant task state information. By representing the task as an information distribution, the definition can be abstracted away from the specific motion trajectories. This representation naturally accommodates uncertainty due to trajectory variability and multiple task solutions. Using this representation, I use an information-based metric &lt;em&gt;ergodicity&lt;/em&gt; to define an objective function with model-based predictive control to generate controls that successfully accomplishes the task with the most efficient motion, given the system dynamics and initial conditions.&lt;/p&gt;
&lt;p&gt;For more information about this project, check out the &lt;a href=&#34;https://murpheylab.github.io/pdfs/2016WAFRPrMaScMu.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
