[{"authors":["admin"],"categories":null,"content":"I am an incoming Assistant Professor at the University of Sydney in the Department of Aerospace, Mechanical and Mechatronic Engineering. My research explores human-robot interaction and collaboration through robot learning and optimal control. I focus on developing useful algorithmic interfaces for humans to intuitively interact with and control complex, dynamic robotic systems. Previously, I was a postdoctoral researcher in the Learning Algorithms and Systems (LASA) lab at EPFL, where I worked on multimodal sensory learning for safe manipulation and adaptive safety controllers for human-robot collaboration. I obtained my Ph.D. at Northwestern University, where my research focused on algorithms for intuitive human-robot collaboration and efficient robot learning. I led Northwestern\u0026rsquo;s team for the DARPA OFFSET Urban Swarm Challenge, developing autonomous swarm algorithms for shared human-swarm collaboration under dynamic, time-sensitive constraints. Prior to joining the University of Sydney, I served as an Associate Research Scientist and Lecturer at Yale University in the Department of Mechanical Engineering.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://apr600.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an incoming Assistant Professor at the University of Sydney in the Department of Aerospace, Mechanical and Mechatronic Engineering. My research explores human-robot interaction and collaboration through robot learning and optimal control. I focus on developing useful algorithmic interfaces for humans to intuitively interact with and control complex, dynamic robotic systems. Previously, I was a postdoctoral researcher in the Learning Algorithms and Systems (LASA) lab at EPFL, where I worked on multimodal sensory learning for safe manipulation and adaptive safety controllers for human-robot collaboration.","tags":null,"title":"Ahalya Prabhakar","type":"authors"},{"authors":[],"categories":[],"content":"I\u0026rsquo;m excited to announce I\u0026rsquo;ll be starting as an Assistant Professor at the University of Sydney in the Department of Aerospace, Mechanical, and Mechatronic Engineering in January 2026!\n","date":1759449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1759449600,"objectID":"20cbc2f87b0c56e6a5041eaff09d26bf","permalink":"https://apr600.github.io/post/usyd-start/","publishdate":"2025-10-03T00:00:00Z","relpermalink":"/post/usyd-start/","section":"post","summary":"I\u0026rsquo;m excited to announce I\u0026rsquo;ll be starting as an Assistant Professor at the University of Sydney in the Department of Aerospace, Mechanical, and Mechatronic Engineering in January 2026!","tags":[],"title":"Starting as Assistant Professor at University of Sydney!","type":"post"},{"authors":["L. Niederhauser","A. Prabhakar","D. Reber","A. Billard"],"categories":[],"content":"","date":1670778683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670778683,"objectID":"3866f613e141f421a19a09e9a9985155","permalink":"https://apr600.github.io/publication/multimodal-manip-esn/","publishdate":"2022-12-11T12:11:23-05:00","relpermalink":"/publication/multimodal-manip-esn/","section":"publication","summary":"Robust in-hand manipulation of objects with movable content requires estimation and prediction of the contents' motion with enough anticipation to allow time to compensate for resulting internal torques. The quick estimation of the objects' dynamics can be challenging when the objects' motion properties (e.g., type, amount, dynamics) cannot be observed visually due to robot occlusions or opacity of the container. This can be further complicated by the computational requirements of onboard hardware available for real-time processing and control for robotics. In this work, we develop a simple learning framework that uses echo state networks to predict the torques experienced on the robotic hand with enough anticipation to allow for adaptive controls and sufficient efficiency for real-time prediction without GPU processing. We demonstrate the efficacy of this formulation for tactile force prediction on the Allegro robotic hand with a Tekscan tactile skin using both material-specific and material-agnostic learned models. We show that while both are effective, the material-specific models show an improvement in accuracy due to the difference in inertial properties between the different materials. We also develop a prediction model that uses audio feedback to augment the tactile predictions. We show that adding auditory feedback improves the prediction error, though it significantly increases the computation cost of the model. We validate this formulation for online prediction on the robotic hand moving materials in real-time and adapting grip for slip detection.","tags":[],"title":"A Predictive Model for Tactile Force Estimation using Audio-Tactile Data","type":"publication"},{"authors":["J. Ketchum","A. Prabhakar","T. D. Murphey"],"categories":[],"content":"","date":1670778683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670778683,"objectID":"d68dacc59726247bfbeaed646b6cced5","permalink":"https://apr600.github.io/publication/tactile-learning-icra/","publishdate":"2022-12-11T12:11:23-05:00","relpermalink":"/publication/tactile-learning-icra/","section":"publication","summary":"Tactile perception is important for robotic systems that interact with the world through touch. Touch is an active sense in which tactile measurements depend on the contact properties of an interaction--e.g., velocity, force, acceleration--as well as properties of the sensor and object under test. These dependencies make training tactile perceptual models challenging. Additionally, the effects of limited sensor life and the near-field nature of tactile sensors preclude the practical collection of exhaustive data sets even for fairly simple objects. Active learning provides a mechanism for focusing on only the most informative aspects of an object during data collection. Here we employ an active learning approach that uses a data-driven model's entropy as an uncertainty measure and explore relative to that entropy conditioned on the sensor state variables. Using a coverage-based ergodic controller, we train perceptual models in near-real time. We demonstrate our approach using a biomimentic sensor, exploring ``tactile scenes'' composed of shapes, textures, and objects. Each learned representation provides a perceptual sensor model for a particular tactile scene. Models trained on actively collected data outperform their randomly collected counterparts in real-time training tests. Additionally, we find that the resulting network entropy maps can be used to identify high salience portions of a tactile scene.","tags":[],"title":"Active Exploration for Real-Time Haptic Training","type":"publication"},{"authors":["M. Schlafly","A. Prabhakar","K. Popović","G. Schlafly","C. Kim","T. D. Murphey"],"categories":[],"content":"","date":1670778683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670778683,"objectID":"4ab4a33a0d4388bc69013117338769ed","permalink":"https://apr600.github.io/publication/swarm-hsr-pnas/","publishdate":"2022-12-11T12:11:23-05:00","relpermalink":"/publication/swarm-hsr-pnas/","section":"publication","summary":"Despite theoretical benefits of collaborative robots, disappointing outcomes are well documented by clinical studies, spanning rehabilitation, prostheses, and surgery. Cognitive load theory provides a possible explanation for why humans in the real world are not realizing the benefits of collaborative robots: high cognitive loads may be impeding human performance. Measuring cognitive availability using an electrocardiogram, we ask 25 participants to complete a virtual-reality task alongside an invisible agent that determines optimal performance by iteratively updating the Bellman equation. Three robots assist by providing environmental information relevant to task performance. By enabling the robots to act more autonomously—managing more of their own behavior with fewer instructions from the human—here we show that robots can augment participants’ cognitive availability and decision-making. The way in which robots describe and achieve their objective can improve the human’s cognitive ability to reason about the task and contribute to human–robot collaboration outcomes. Augmenting human cognition provides a path to improve the efficacy of collaborative robots. By demonstrating how robots can improve human cognition, this work paves the way for improving the cognitive capabilities of first responders, manufacturing workers, surgeons, and other future users of collaborative autonomy systems.","tags":[],"title":"Collaborative robots can augment human cognition in regret-sensitive tasks","type":"publication"},{"authors":["K. Popović","M. Schlafly","A. Prabhakar","C. Kim","T. D. Murphey"],"categories":[],"content":"","date":1670778683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670778683,"objectID":"0eb0675c4015c24be7ac7c321a625f27","permalink":"https://apr600.github.io/publication/swarm-hsr-iros/","publishdate":"2022-12-11T12:11:23-05:00","relpermalink":"/publication/swarm-hsr-iros/","section":"publication","summary":"During a natural disaster such as hurricane, earthquake, or fre, robots have the potential to explore vast areas and provide valuable aid in search \u0026 rescue efforts. These scenarios are often high-pressure and time-critical with dynamicallychanging task goals. One limitation to these large scale deployments is effective human-robot interaction. Prior work shows that collaboration between one human and one robot benefits from shared control. Here we evaluate the effcacy of shared control for human-swarm teaming in an immersive virtual reality environment. Although there are many human-swarminteraction paradigms, few are evaluated in high-pressure settings representative of their intended end use. We have developed an open-source virtual reality testbed for realistic evaluation of human-swarm teaming performance under pressure. We conduct a user study (n=16) comparing four human-swarm paradigms to a baseline condition with no robotic assistance. Shared control signifcantly reduces the number of instructions needed to operate the robots. While shared control leads to marginally improved team performance in experienced participants, novices perform best when the robots are fully autonomous. Our experimental results suggest that in immersive, high-pressure settings, the benefts of robotic assistance may depend on how the human and robots interact and the human operator’s expertise.","tags":[],"title":"Measuring Human-Robot Team Benefits Under Time Pressure in a Virtual Reality Testbed","type":"publication"},{"authors":["A. Prabhakar","T. D. Murphey"],"categories":[],"content":"","date":1670778683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670778683,"objectID":"61b1e0735e92f83e9652855758c93dfa","permalink":"https://apr600.github.io/publication/mechanical-intelligence/","publishdate":"2022-12-11T12:11:23-05:00","relpermalink":"/publication/mechanical-intelligence/","section":"publication","summary":"Intelligence involves processing sensory experiences into representations useful for prediction. Understanding sensory experiences and building these contextual representations without prior knowledge of sensor models and environment is a challenging unsupervised learning problem. Current machine learning methods process new sensory data using prior knowledge defined by either domain knowledge or datasets. When datasets are not available, data acquisition is needed, though automating exploration in support of learning is still an unsolved problem. Here we develop a method that enables agents to efficiently collect data for learning a predictive sensor model—without requiring domain knowledge, human input, or previously existing data—using ergodicity to specify the data acquisition process. This approach is based entirely on data-driven sensor characteristics rather than predefined knowledge of the sensor model and its physical characteristics. We learn higher quality models with lower energy expenditure during exploration for data acquisition compared to competing approaches, including both random sampling and information maximization. In addition to applications in autonomy, our approach provides a potential model of how animals use their motor control to develop high quality models of their sensors (sight, sound, touch) before having knowledge of their sensor capabilities or their surrounding environment.","tags":[],"title":"Mechanical intelligence for learning embodied sensor-object relationships","type":"publication"},{"authors":["J. Meyer","A. Prabhakar","A. Pinosky","I. Abraham","A. Taylor","M. Schlafly","K. Popovic","G. Diniz","B. Teich","B. Simidchieva","S. Clark","T. D. Murphey"],"categories":[],"content":"","date":1670778683,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670778683,"objectID":"78561a9e1252dc2301645b2145a24362","permalink":"https://apr600.github.io/publication/scale-invariant-darpa/","publishdate":"2022-12-11T12:11:23-05:00","relpermalink":"/publication/scale-invariant-darpa/","section":"publication","summary":"We present a method for controlling a swarm using its spectral decomposition—that is, by describing the set of trajectories of a swarm in terms of a spatial distribution throughout the operational domain—guaranteeing scale invariance with respect to the number of agents both for computation and for the operator tasked with controlling the swarm. We use ergodic control, decentralized across the network, for implementation. In the DARPA OFFSET program field setting, we test this interface design for the operator using the STOMP interface—the same interface used by Raytheon BBN throughout the duration of the OFFSET program. In these tests, we demonstrate that our approach is scale-invariant—the user specification does not depend on the number of agents; it is persistent—the specification remains active until the user specifies a new command; and it is real-time— the user can interact with and interrupt the swarm at any time. Moreover, we show that the spectral/ergodic specification of swarm behavior degrades gracefully as the number of agents goes down, enabling the operator to maintain the same approach as agents become disabled or are added to the network. We demonstrate the scale invariance and dynamic response of our system in a field-relevant simulator on a variety of tactical scenarios with up to 50 agents. We also demonstrate the dynamic response of our system in the field with a smaller team of agents. Lastly, we make the code for our system available.","tags":[],"title":"Scale-Invariant Specifications for Human-Swarm Systems","type":"publication"},{"authors":["A. Prabhakar","A. Billard"],"categories":[],"content":"","date":1639069885,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639069885,"objectID":"ffc4868674b7ef9ebbe982041d3e9644","permalink":"https://apr600.github.io/publication/ai-hri-safe-irl/","publishdate":"2020-05-13T12:12:24-05:00","relpermalink":"/publication/ai-hri-safe-irl/","section":"publication","summary":"A critical need in assistive robotics, such as assistive wheelchairs for navigation, is a need to learn task intent and safety guarantees through user interactions in order to ensure safe task performance. For tasks where the objectives from the user are not easily defined, learning from user demonstrations has been a key step in enabling learning. However, most robot learning from demonstration (LfD) methods primarily rely on optimal demonstration in order to successfully learn a control policy, which can be challenging to acquire from novice users. Recent work does use suboptimal and failed demonstrations to learn about task intent; few focus on learning safety guarantees to prevent repeat failures experienced, essential for assistive robots. Furthermore, interactive human-robot learning aims to minimize effort from the human user to facilitate deployment in the real-world. As such, requiring users to label the unsafe states or keyframes from the demonstrations should not be a necessary requirement for learning. Here, we propose an algorithm to learn a safety value function from a set of suboptimal and failed demonstrations that is used to generate a real-time safety control filter. Importantly, we develop a credit assignment method that extracts the failure states from the failed demonstrations without requiring human labelling or prespecified knowledge of unsafe regions. Furthermore, we extend our formulation to allow for user-specific safety functions, by incorporating user-defined safety rankings from which we can generate safety level sets according to the users’ preferences. By using both suboptimal and failed demonstrations and the developed credit assignment formulation, we enable learning a safety value function with minimal effort needed from the user, making it more feasible for widespread use in human-robot interactive learning tasks.","tags":[],"title":"Credit Assignment Safety Learning from Human Demonstrations","type":"publication"},{"authors":["A. Prabhakar","S. Furrer","L. Panchetti","M. Perret","A. Billard"],"categories":[],"content":"","date":1639069884,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639069884,"objectID":"d95faccc59b622856f25704dccc15414","permalink":"https://apr600.github.io/publication/ai-hri-sensory/","publishdate":"2020-05-13T12:11:23-05:00","relpermalink":"/publication/ai-hri-sensory/","section":"publication","summary":"Adaptive control for real-time manipulation requires quick estimation and prediction of object properties. While robot learning in this area primarily focuses on using vision, many tasks cannot rely on vision due to object occlusion. Here, we formulate a learning framework that uses multimodal sensory fusion of tactile and audio data in order to quickly characterize and predict an object’s properties. The predictions are used in a developed reactive controller to adapt the grip on the object to compensate for the predicted inertial forces experienced during motion. Drawing inspiration from how humans interact with objects, we propose an experimental setup from which we can understand how to best utilize different sensory signals and actively interact with and manipulate objects to quickly learn their object properties for safe manipulation.","tags":[],"title":"Multimodal Sensory Learning for Real-time, Adaptive Manipulation","type":"publication"},{"authors":["A. Prabhakar*","A. Kalinowska*","K. Fitzsimons and T.D. Murphey"],"categories":[],"content":"","date":1639069883,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639069883,"objectID":"37384d424ec72d3ca3a36992ea8e115f","permalink":"https://apr600.github.io/publication/ergodic-lfd/","publishdate":"2020-05-13T12:11:23-05:00","relpermalink":"/publication/ergodic-lfd/","section":"publication","summary":"With growing access to versatile robotics, it is beneficial for end users to be able to teach robots tasks without needing to code a control policy. One possibility is to teach the robot through successful task executions. However, nearoptimal demonstrations of a task can be difficult to provide and even successful demonstrations can fail to capture task aspects key to robust skill replication. Here, we propose a learning from demonstration (LfD) approach that enables learning of robust task definitions without the need for near-optimal demonstrations. We present a novel algorithmic framework for learning tasks based on the ergodic metric—a measure of information content in motion. Moreover, we make use of negative demonstrations—demonstrations of what not to do—and show that they can help compensate for imperfect demonstrations, reduce the number of demonstrations needed, and highlight crucial task elements improving robot performance. In a proofof-concept example of cart-pole inversion, we show that negative demonstrations alone can be sufficient to successfully learn and recreate a skill. Through a human subject study with 24 participants, we show that consistently more information about a task can be captured from combined positive and negative (posneg) demonstrations than from the same amount of just positive demonstrations. Finally, we demonstrate our learning approach on simulated tasks of target reaching and table cleaning with a 7-DoF Franka arm. Our results point towards a future with robust, data-efficient LfD for novice users.","tags":[],"title":"Ergodic imitation: Learning from what to do and what not to do","type":"publication"},{"authors":["A. Prabhakar","I. Abraham","A. Taylor","M. Schlafly","K. Popovic","G. Diniz","B. Teich","B. Simidchieva","S.Clark  and T.D. Murphey"],"categories":[],"content":"","date":1589389883,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589389883,"objectID":"678574a64b85f89b044c78023395537c","permalink":"https://apr600.github.io/publication/darpa-fx3-rss/","publishdate":"2020-05-13T12:11:23-05:00","relpermalink":"/publication/darpa-fx3-rss/","section":"publication","summary":"This paper presents a formulation for swarm control and high-level task planning that is dynamically responsive to user commands and adaptable to environmental information. We design an end-to-end pipeline from a tactile tablet interface for user commands to onboard control of robotic agents based on decentralized ergodic coverage. Our approach demonstrates reliable and dynamic control of a swarm collective through the use of ergodic specifications for planning and executing agent trajectories as well as responding to user and external inputs. We validate our approach in a virtual reality simulation environment and in real-world experiments with a robotic swarm where user-based control of the swarm and mission-based tasks require a dynamic and flexible response to changing conditions and objectives in real-time.","tags":[],"title":"Ergodic Specifications for Flexible Swarm Control: From User Commands to Persistent Adaptation","type":"publication"},{"authors":["I. Abraham","A. Prabhakar and T.D. Murphey"],"categories":[],"content":"","date":1589389531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589389531,"objectID":"37c2b7054b4700c9f804122034bf115e","permalink":"https://apr600.github.io/publication/kl-e3-tase/","publishdate":"2020-05-13T12:05:31-05:00","relpermalink":"/publication/kl-e3-tase/","section":"publication","summary":"This paper develops KL-Ergodic Exploration from Equilibrium (KL-E3), a method for robotic systems to integrate stability into actively generating informative measurements through ergodic exploration. Ergodic exploration enables robotic systems to indirectly sample from informative spatial distributions globally, avoiding local optima, and without the need to evaluate the derivatives of the distribution against the robot dynamics. Using hybrid systems theory, we derive a controller that allows a robot to exploit equilibrium policies (i.e., policies that solve a task) while allowing the robot to explore and generate informative data using an ergodic measure that can extend to high-dimensional states. We show that our method is able to maintain Lyapunov attractiveness with respect to the equilibrium task while actively generating data for learning tasks such, as Bayesian optimization, model learning, and off-policy reinforcement learning. In each example, we show that our proposed method is capable of generating an informative distribution of data while synthesizing smooth control signals. We illustrate these examples using simulated systems and provide simplification of our method for real-time online learning in robotic systems.","tags":[],"title":"Dynamic Coverage for Active Learning From Equilibrium","type":"publication"},{"authors":[],"categories":[],"content":"As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable. Furthermore, in high-dimensional sensor systems, I explore how we can efficiently acquire and utilize the data for model learning and communication.\nMultimodal Sensory Learning for Real-time Adaptive Manipulation Adaptive control for real-time manipulation requires quick estimation and prediction of object properties. While robot learning in this area primarily focuses on using vision, many tasks cannot rely on vision due to object occlusion. I focus on developing methods for using multimodal sensory fusion of tactile and audio data to quickly characterize and predict an object\u0026rsquo;s tactile force during motion dependent on their inertial properties. The predictions are used in a adaptive grasp controller to compensate for the predicted inertial forces experienced during motion. Drawing inspiration from how humans interact with objects, I investigate how to best utilize different sensory signals and actively interact with and manipulate objects to quickly learn their object properties for safe manipulation. By exploring the information content and quality in different sensory modalities, I develop algorithms that autonomously extract the relevant task information to maintain grasp stability during manipulation.\nFor more information, check out the paper on this work.\nMechanical intelligence for learning embodied sensor-object relationships My work explores the idea of active learning for high-dimensional sensory spaces. Understanding sensory experiences and building contextual representations without prior knowledge of sensor models and environment is a challenging unsupervised learning problem. This is particularly challenging for robotics because the data has to be physically acquired and affects the learning process. I develop a method that enables agents to efficiently collect data for learning a predictive sensor model—without requiring domain knowledge, human input, or previously existing data—using ergodicity to specify the data acquisition process. This approach is based entirely on data-driven sensor characteristics rather than predefined knowledge of the sensor model and its physical characteristics.\nFor more information, check out the paper on this work.\nActive Exploration for Learning Haptic Features My work explores the idea of active learning for tactile exploration. Tactile exploration is a particularly interesting example of high-dimensional sensory exploration because, unlike vision, the exploratory motions used to obtain the data affect the data itself. That is, how you move across the texture affects the sensory data received. As such, learning and classifying texture data from exploratory motions can be a challenging problem.\nMost of the work in tactile exploration either focus on learning to translate the sensory signals to physically interpretable characteristics. Others simplify the tactile exploration for generating discriminatory motions by either mimicking human exploratory motions or by building a library of motions to discriminate particular characteristics. The question I am exploring is, Can I learn textures and mappings of workspaces directly from the high-dimensional tactile sensory signals without trying to associate physical characteristics to the sensory signals?\nUsing the Biotac tactile sensor from SynTouch, Inc., a 19-dimensional tactile sensor that mimics the sensory capabilities of the human fingertip, I explore the use of unsupervised learning methods to learn, model, and classify textures without a prior library of texture definitions or predefined exploratory motions.\nFor more information, check out the paper on this work.\nDistribution-Based Active Exploration and the KL-Divergence Measure KL-E3 quadcopter (in blue) actively acquiring data to learn its stochastic model compared to a quadcopter acquiring data using an information-maximizing method (in green). K-L divergence based control method focused on efficiently exploring high-dimensional search spaces safely for active, stable exploration. For efficient learning, we want to drive the exploration of the robot to prioritize obtaining more informative data. To do this, we defined an information distribution over the search space that represented the importance measure of the data (such as measures of linear independence from sparse Gaussian processes or measures representing model parameter uncertainty with respect to the search space). By defining the importance measure based on the state space, we generated a representation of the state space that prioritized states that generate highly informative data.\nUsing the KL-E3 method to actively acquire data to learn and match a distribution. We then used information-based measures with hybrid control to generate controls that cover the search space and prioritizing high-information regions by matching the information distribution generated, while maintaining safety and stability constraints. The ergodic metric allows us to quantitatively define a measure that minimizes the distance between the information distribution and the time-averaged trajectory of the robot. Using it in the cost function allows us to generate controls that explore the full state-space while prioritizing high-information regions. The main limitation of the ergodic metric used in the active exploration algorithm is that the computational complexity of ergodicity exponentially increases with the dimensionality of the search space. To address this, we derive an active exploration algorithm using a sampling-based approximation of the ergodic metric using the Kullback-Leibler divergence metric. The sampling-based approximation allowed us to actively explore a much higher dimensional search space without sacrificing real-time control.\nFor more information about this project, check out the papers on this work here and here and its Github repository.\nA robot using the KL-E3 method to sampling a state space (for Bayesian optimization) while taking into account dynamic constraints (keeping the cart double pendulum inverted). ","date":1567530454,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567530454,"objectID":"9c9a7431a41176a037478bfd19fe85c3","permalink":"https://apr600.github.io/project/perception-model-learning/","publishdate":"2019-09-03T12:07:34-05:00","relpermalink":"/project/perception-model-learning/","section":"project","summary":"As robotic systems are deployed with varying sensor modalities, especially in novel scenarios, it is important for the robot to build models of how it interacts in its environment, e.g., a action- perception model, and be able to communicate its role and capabilities. I aim to develop methods that enable robots to autonomously construct meaningful sensor-environment models that are physically-driven (i.e., an model of their world and how their actions affect their perception of it) and can encode multimodal information into latent representations that are human-interpretable.","tags":[],"title":"Learning Interpretable Action-Perception Models","type":"project"},{"authors":[],"categories":[],"content":"For robot interfaces to be practical for humans, they must be simple, seamless, and mitigate cognitive load for the user. This is particularly critical for users who may not have familiarity with robotic systems or in settings with complex collaborative robotic systems (e.g., robot swarms) or dynamically changing environments, where managing the system can be overwhelming. In my research, I seek to design user interfaces for human-robot collaboration that effectively communicates information to the human with minimal cognitive load and training to use.\nErgodic Specifications for Flexible Swarm Control and Dynamic Task Adaptation People often have to perform a variety of tasks-including search-and-rescue, target location, or exploration and terrain mapping- in unfamiliar environments. Drones deployed in the field with them have the ability to greatly improve their situational and perceptual awareness by providing feedback to aid in their task performance and safety. However, swarm deployment can be complicated, as drones can be difficult for a person to control and operate without greatly increasing their cognitive load, which can make task performance difficult and inefficient. With this in mind, I ask the question: How can we design a human-swarm system to best accomplish a task under pressure?\nSimulation of a swarm dynamically adapting to the environment while also responding to user commands. (a) When the swarm discovers a DD, the agents cover the rest of the workspace while avoiding that location. (b) When a user inputs a bimodal distribution (shown as the dark region on the map), the swarm responds to the user commands, while continuing to avoid the DD location. (c) When the swarm finds an EE, it simultaneously converges on the EE, covers the user inputs, and avoids the DD location. (d) shows the resulting target distribution for the combined tasks. I led the Northwestern team on the DARPA FX-3 Urban Swarm Challenge project, exploring this idea. I developed a formulation for swarm control and high-level task planning that is dynamically responsive to user commands and adaptable to environmental information. I designed an end-to-end pipeline from a tactile tablet interface for user commands to onboard control of robotic agents based on decentralized ergodic coverage. I conducted experiments with a robotic swarm at the DARPA OFFSET FX3 field tests, combining user inputs and task specifications to generate swarm behavior flexible to changing conditions and objectives in real-time. I also developed an experimental VR test bed to validate our approach in a simulation and conduct human subject studies investigating human-robot collaboration under pressure.\nThe Tanvas tactile tablet allows the operator to communicate their preferences for coverage to the swarm. User inputs are incorporated through a tablet interface we developed for communicating regions of interest by the user to a swarm in realtime using the TanvasTouch monitor. Using the TanvasTouch, the user can specify regions of exploratory interest by simply shading the regions of interest on the TanvasTouch. The tablet interface transmits a set of desired points on the workspace for the swarm to prioritize and the spatial distribution is generated by assigning the highest priority value at each of those points in a discretized workspace. The user-specified distribution is combined with the task-based distribution to generate swarm control that is dynamically responsive to both task updates and user inputs.\nFor more information, check out the following papers (paper1, paper2) on this work.\nVirtual Reality System for Human-Subject Studies We developed an experimental urban environment testbed using the Unity game engine and using an HTC Vive for controlling operator movement inside the virtual reality environment. As the operator moved within the VR environment, a swarm of simulated quadrotors running the ergodic swarm algorithm provided assisstane to the operator in real time.The swarm\u0026rsquo;s behavior was governed by ergodic task specifications as described above, as well as using the TanvasTouch haptic interface to send commands for desired areas of explorations to the swarm.\nThe VR environment was used to validate the full system architecture of the shared ergodic formulation with multiple two-way communication channels. In addition, we have conducted user studies using this testbed to analyze the impact of the shared ergodic swarm control approach compared to standard methods and analyzes the effects of different ergodic specifications within our framework on task performance under dynamic, time-sensitive constraints.\nFor more information, check out the following papers (paper1, paper2) on this work.\n","date":1567530434,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567530434,"objectID":"b8ff3a0d4f5ac472a805404dcd42fdd7","permalink":"https://apr600.github.io/project/human-swarm-collab/","publishdate":"2019-09-03T12:07:14-05:00","relpermalink":"/project/human-swarm-collab/","section":"project","summary":"Designing interpretable, multimodal interfaces for intuitive human-robot interaction and collaboration","tags":[],"title":"Optimal Interface Design for Human-Robot Collaboration","type":"project"},{"authors":[],"categories":[],"content":"Enabling seamless human-robot collaboration while ensuring task success requires the reduction of the task information to its essential features through representations that are intuitive for the user. To do this, I focus on developing methods for generating and learning task representations from motion data and information that can be incorporated into model-based control methods. My goal is to develop methods that enable autonomy to extract understandable task embeddings that enable control methods to ensure dynamic feasibility and provably safe behavior and can be communicated to and interpreted by humans, a step in developing explainable AI.\nCredit Assignment Safety Learning from Suboptimal and Failure Demonstrations A critical need in assistive robotics is to learn task intent and safety guarantees through user interactions in order to ensure safe task performance. Most robot learning from demonstration (LfD) and inverse reinforcement learning (IRL) methods primarily rely on optimal demonstration in order to successfully learn a control policy, which can be challenging to acquire from novice users. Recent work does use suboptimal and failed demonstrations to learn about task intent; few focus on learning safety guarantees to prevent repeat failures experienced, essential for assistive robots. Furthermore, interactive human-robot learning aims to minimize effort from the human user to facilitate deployment in the real-world. As such, requiring users to label the unsafe states or keyframes from the demonstrations should not be a necessary requirement for learning. I developed an algorithm to learn a safety value function from a set of suboptimal and failed demonstrations that is used to generate a real-time safety control filter. Importantly, we develop a credit assignment method that extracts the failure states from the failed demonstrations without requiring human labelling or prespecified knowledge of unsafe regions. This method can be combined with standard LfD or IRL methods to learn a task policy that also guarantees safety during execution.\nFor more information about this project, check out the paper or my talk.\nLearning from Variable, Imperfect Demonstrations using Ergodic Control As robots become more ubiquitous in every day life, they will be interacting with people more and will need to learn from them. At the same time, as regular people are required to teach robots to perform more challenging tasks and provide demonstrations for complicated robotic systems that they may be unfamiliar with, they may provide demonstrations that are non-optimal or even unsuccessful. How do we allow robots to learn a task representation from a set of human demonstrations that 1) can encompass varied solutions to the same task, and, perhaps more importantly, 2) can still generate optimal solutions from a set of imperfect demonstrations from a non-expert user?\nHere, I explore the idea of representing the set of demonstrations as an information distribution over the task space. By considering each demonstration as adding information about the task, we can consider imperfect, and even unsuccessful, demonstrations as still adding valuable information about the task to the representation. As such, a set of imperfect demonstrations can collectively create a task representation from which we can generate controls for optimal task performance. Furthermore, this representation allows more flexibility in the demonstration set, allowing for multiple solution sets for the same task. By representing the task as information over the task space, multiple solutions can emerge from a demonstration set and variations that are irrelevant to task success will naturally be averaged out.\nFor more information about this project, check out the paper.\nRobotic Visual Rendering using Information-Theoretic Methods Drawing is a classic example of tasks where the motions are used to successfully accomplish the task of communicating information. It falls into a unique subset of tasks where the motion is simultaneously essential for accomplishing the task, but there are many motion trajectories that can successfully accomplish it\u0026mdash; people may draw the same image completely different, but they ultimately result in the same image. I wanted to give the same level of robustness and generality to a robot\u0026rsquo;s task performance.\nTo do this, I represent the task as the distribution over the state space representing the relevant task state information. By representing the task as an information distribution, the definition can be abstracted away from the specific motion trajectories. This representation naturally accommodates uncertainty due to trajectory variability and multiple task solutions. Using this representation, I use an information-based metric ergodicity to define an objective function with model-based predictive control to generate controls that successfully accomplishes the task with the most efficient motion, given the system dynamics and initial conditions.\nFor more information about this project, check out the paper.\n","date":1567530245,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567530245,"objectID":"ec1d384fbf4620e77f6dc6ca15370b11","permalink":"https://apr600.github.io/project/task-rep/","publishdate":"2019-09-03T12:04:05-05:00","relpermalink":"/project/task-rep/","section":"project","summary":"Enabling seamless human-robot collaboration while ensuring task success requires the reduction of the task information to its essential features through representations that are intuitive for the user. To do this, I focus on developing methods for generating and learning task representations from motion data and information that can be incorporated into model-based control methods. My goal is to develop methods that enable autonomy to extract understandable task embeddings that enable control methods to ensure dynamic feasibility and provably safe behavior and can be communicated to and interpreted by humans, a step in developing explainable AI.","tags":[],"title":"Principled Methods for Human-Robot Collaborative Learning and Control","type":"project"},{"authors":["I. Abraham","A. Prabhakar and T.D. Murphey"],"categories":[],"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"27b33764220d2ad54a2017aa1a8363fa","permalink":"https://apr600.github.io/publication/kl-e3-wafr/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/publication/kl-e3-wafr/","section":"publication","summary":"This paper develops a method for robots to integrate stability into actively seeking out informative measurements through coverage. We derive a controller using hybrid systems theory that allows us to consider safe equilibrium policies during active data collection. We show that our method is able to maintain Lyapunov attractiveness while still actively seeking out data. Using incremental sparse Gaussian processes, we define distributions which allow a robot to actively seek out informative measurements. We illustrate our methods for shape estimation using a cart double pendulum, dynamic model learning of a hovering quadrotor, and generating galloping gaits starting from stationary equilibrium by learning a dynamics model for the half-cheetah system from the Roboschool environment.","tags":[],"title":"Active Area Coverage from Equilibrium","type":"publication"},{"authors":["I. Abraham","A. Prabhakar","and T.D. Murphey"],"categories":[],"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"cb85bf3b71de1966c24d3fe7e9dfe3fa","permalink":"https://apr600.github.io/publication/ergodic-shape-estimation/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/publication/ergodic-shape-estimation/","section":"publication","summary":"Current methods to estimate object shape—using either vision or touch—generally depend on high-resolution sensing. Here, we exploit ergodic exploration to demonstrate successful shape estimation when using a low-resolution binary contact sensor. The measurement model is posed as a collisionbased tactile measurement, and classification methods are used to discriminate between shape boundary regions in the search space. Posterior likelihood estimates of the measurement model help the system actively seek out regions where the binary sensor is most likely to return informative measurements. Results show successful shape estimation of various objects as well as the ability to identify multiple objects in an environment. Interestingly, it is shown that ergodic exploration utilizes non-contact motion to gather significant information about shape. The algorithm is extended in three dimensions in simulation and we present two dimensional experimental results using the Rethink Baxter robot.","tags":[],"title":"Ergodic Exploration using Binary Sensing for Non-Parametric Shape Estimation","type":"publication"},{"authors":["A. Prabhakar","A. Mavrommati","J. Schultz","and T.D. Murphey"],"categories":[],"content":"","date":1481760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481760000,"objectID":"af41ac05768764891c6f425b6113c634","permalink":"https://apr600.github.io/publication/ergodic-drawing-wafr/","publishdate":"2016-12-15T00:00:00Z","relpermalink":"/publication/ergodic-drawing-wafr/","section":"publication","summary":". This paper addresses the problem of enabling a robot to represent and recreate visual information through physical motion, focusing on drawing using pens, brushes, or other tools. This work uses ergodicity as a control objective that translates planar visual input to physical motion without preprocessing (e.g., image processing, motion primitives). We achieve comparable results to existing drawing methods, while reducing the algorithmic complexity of the software. We demonstrate that optimal ergodic control algorithms with different time-horizon characteristics (infinitesimal, finite, and receding horizon) can generate qualitatively and stylistically different motions that render a wide range of visual information (e.g., letters, portraits, landscapes). In addition, we show that ergodic control enables the same software design to apply to multiple robotic systems by incorporating their particular dynamics, thereby reducing the dependence on task-specific robots. Finally, we demonstrate physical drawings with the Baxter robot.","tags":[],"title":"Autonomous Visual Rendering using Physical Motion","type":"publication"},{"authors":["G. De La Torre","K. Flaskamp","A. Prabhakar","and T.D. Murphey"],"categories":[],"content":"","date":1456790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456790400,"objectID":"0807741b279201ce203fadbfa61bc668","permalink":"https://apr600.github.io/publication/stochastic-ergodic-acc/","publishdate":"2016-03-01T00:00:00Z","relpermalink":"/publication/stochastic-ergodic-acc/","section":"publication","summary":"Ergodic exploration has been shown to be an effective framework for autonomous sensing and exploration. The objective of ergodic control is to minimize the difference between the distribution of the time-averaged sensor trajectory and a spatial probability distribution function representing information density. Therefore, the time a sensor spends sampling a particular region is manipulated to correspond to the anticipated information density of that region. This paper introduces a trajectory optimization approach for ergodic exploration in the presence of stochastic sensor dynamics. The stochastic differential dynamic programming algorithm is formulated in the context of ergodic exploration. Numerical studies demonstrate the proposed framework’s ability to mitigate stochastic effects.","tags":[],"title":"Ergodic Exploration with Stochastic Sensor Dynamics","type":"publication"},{"authors":["A. Prabhakar","K. Flaskamp","and T.D. Murphey"],"categories":[],"content":"","date":1450137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1450137600,"objectID":"fdc312b21a3546be0ae674bf0c98f742","permalink":"https://apr600.github.io/publication/symplectic-ergodic-control/","publishdate":"2015-12-15T00:00:00Z","relpermalink":"/publication/symplectic-ergodic-control/","section":"publication","summary":"Autonomous active exploration requires search algorithms that can effectively balance the need for workspace coverage with energetic costs. We present a strategy for planning optimal search trajectories with respect to the distribution of expected information over a workspace. We formulate an iterative optimal control algorithm for general nonlinear dynamics, where the metric for information gain is the difference between the spatial distribution and the statistical representation of the time-averaged trajectory, i.e. ergodicity. Previous work has designed a continuous-time trajectory optimization algorithm. In this paper, we derive two discrete-time iterative trajectory optimization approaches, one based on standard first order discretization and the other using symplectic integration. The discrete-time methods based on first-order discretization techniques are both faster than the continuous-time method in the studied examples. Moreover, we show that even for a simple system, the choice of discretization has a dramatic impact on the resulting control and state trajectories. While the standard discretization method turns unstable, the symplectic method, which is structure-preserving, achieves lower values for the objective.","tags":[],"title":"Symplectic Integration for Optimal Ergodic Control","type":"publication"}]